{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-22T18:36:02.137574Z",
     "iopub.status.busy": "2026-02-22T18:36:02.137049Z",
     "iopub.status.idle": "2026-02-22T18:36:04.295030Z",
     "shell.execute_reply": "2026-02-22T18:36:04.294342Z",
     "shell.execute_reply.started": "2026-02-22T18:36:02.137546Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'semeval2026-task12-dataset'...\n",
      "remote: Enumerating objects: 69, done.\u001b[K\n",
      "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
      "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
      "remote: Total 69 (delta 30), reused 51 (delta 15), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (69/69), 6.72 MiB | 19.88 MiB/s, done.\n",
      "Resolving deltas: 100% (30/30), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sooo66/semeval2026-task12-dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T18:36:04.297299Z",
     "iopub.status.busy": "2026-02-22T18:36:04.296869Z",
     "iopub.status.idle": "2026-02-22T18:36:05.568435Z",
     "shell.execute_reply": "2026-02-22T18:36:05.567699Z",
     "shell.execute_reply.started": "2026-02-22T18:36:04.297271Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!\n",
      "\n",
      "ğŸ”¹ Ø±ÙˆÛŒØ¯Ø§Ø¯ Ù‡Ø¯Ù: Videos of the assassination circulated on social media.\n",
      "ğŸ”¹ Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§:\n",
      "   A: The shooter used a handmade gun.\n",
      "   B: Security arrested the suspected gunman, Tetsuya Yamagami.\n",
      "   C: Shinzo Abe became the deputy chief cabinet secretary in the early 2000s.\n",
      "   D: A man fired twice at Shinzo Abe.\n",
      "ğŸ”¹ Ù¾Ø§Ø³Ø® Ø¯Ø±Ø³Øª: D\n",
      "ğŸ”¹ Ø·ÙˆÙ„ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ (Context): 94617 Ú©Ø§Ø±Ø§Ú©ØªØ±\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…ØªÙˆÙ†\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_and_merge_data(questions_path, docs_path):\n",
    "    # Û±. Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø§Ø³Ù†Ø§Ø¯ (docs.json)\n",
    "    with open(docs_path, 'r', encoding='utf-8') as f:\n",
    "        docs_raw = json.load(f)\n",
    "        \n",
    "    # Û². ØªØ¨Ø¯ÛŒÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù„ÛŒØ³Øª Ø¨Ù‡ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø±ÛŒØ¹ Ø¨Ø± Ø§Ø³Ø§Ø³ topic_id\n",
    "    docs_dict = {}\n",
    "    \n",
    "    # Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø¢ÛŒØ§ ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§Ø³Øª\n",
    "    if isinstance(docs_raw, list):\n",
    "        for item in docs_raw:\n",
    "            if not isinstance(item, dict):\n",
    "                continue\n",
    "                \n",
    "            t_id = item.get(\"topic_id\")\n",
    "            if not t_id:\n",
    "                continue\n",
    "                \n",
    "            if t_id not in docs_dict:\n",
    "                docs_dict[t_id] = []\n",
    "                \n",
    "            # Ø­Ø§Ù„Øª Ø§ÙˆÙ„: Ø§Ú¯Ø± Ø¢ÛŒØªÙ… Ø´Ø§Ù…Ù„ Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø§Ø³Ù†Ø§Ø¯ Ø¯Ø± Ú©Ù„ÛŒØ¯ docs Ø¨Ø§Ø´Ø¯\n",
    "            if \"docs\" in item or \"documents\" in item:\n",
    "                inner_docs = item.get(\"docs\", item.get(\"documents\", []))\n",
    "                for d in inner_docs:\n",
    "                    if isinstance(d, dict):\n",
    "                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¯Ø§Ø®Ù„ÛŒ\n",
    "                        text_val = d.get(\"text\", d.get(\"content\", d.get(\"doc\", \"\")))\n",
    "                        docs_dict[t_id].append(text_val)\n",
    "                    elif isinstance(d, str):\n",
    "                        docs_dict[t_id].append(d)\n",
    "            # Ø­Ø§Ù„Øª Ø¯ÙˆÙ…: Ø§Ú¯Ø± Ø®ÙˆØ¯ Ø¢ÛŒØªÙ… Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ ÛŒÚ© Ø³Ù†Ø¯ Ø¨Ø§Ø´Ø¯\n",
    "            else:\n",
    "                text_val = item.get(\"text\", item.get(\"content\", item.get(\"doc\", \"\")))\n",
    "                if text_val:\n",
    "                    docs_dict[t_id].append(text_val)\n",
    "\n",
    "    merged_dataset = []\n",
    "    \n",
    "    # Û³. Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø³ÙˆØ§Ù„Ø§Øª Ùˆ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø§Ø³Ù†Ø§Ø¯\n",
    "    with open(questions_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "                \n",
    "            question_item = json.loads(line.strip())\n",
    "            topic_id = question_item.get(\"topic_id\")\n",
    "            \n",
    "            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù†Ø§Ø¯ Ø§Ø² Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ú©Ù‡ Ø¨Ø§Ù„Ø§ØªØ± Ø³Ø§Ø®ØªÛŒÙ… (Ø§ÛŒÙ† Ø¨Ø§Ø± Ø¨Ø¯ÙˆÙ† Ø®Ø·Ø§)\n",
    "            related_docs = docs_dict.get(topic_id, [])\n",
    "            \n",
    "            # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ùˆ ØªØ±Ú©ÛŒØ¨ Ù…ØªÙˆÙ†\n",
    "            cleaned_docs = [clean_text(doc) for doc in related_docs if doc]\n",
    "            context_text = \" \".join(cleaned_docs)\n",
    "            \n",
    "            # Û´. Ø³Ø§Ø®ØªØ§Ø± Ù†Ù‡Ø§ÛŒÛŒ\n",
    "            merged_item = {\n",
    "                \"id\": question_item.get(\"id\"),\n",
    "                \"topic_id\": topic_id,\n",
    "                \"target_event\": clean_text(question_item.get(\"target_event\")),\n",
    "                \"options\": {\n",
    "                    \"A\": clean_text(question_item.get(\"option_A\")),\n",
    "                    \"B\": clean_text(question_item.get(\"option_B\")),\n",
    "                    \"C\": clean_text(question_item.get(\"option_C\")),\n",
    "                    \"D\": clean_text(question_item.get(\"option_D\"))\n",
    "                },\n",
    "                \"golden_answer\": question_item.get(\"golden_answer\"),\n",
    "                \"context\": context_text \n",
    "            }\n",
    "            merged_dataset.append(merged_item)\n",
    "            \n",
    "    return merged_dataset\n",
    "\n",
    "# ==========================================\n",
    "# Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø¯\n",
    "# ==========================================\n",
    "\n",
    "QUESTIONS_FILE = 'semeval2026-task12-dataset/sample_data/questions.jsonl'\n",
    "DOCS_FILE = 'semeval2026-task12-dataset/sample_data/docs.json'\n",
    "\n",
    "dataset = load_and_merge_data(QUESTIONS_FILE, DOCS_FILE)\n",
    "\n",
    "if dataset:\n",
    "    print(\"âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!\\n\")\n",
    "    print(f\"ğŸ”¹ Ø±ÙˆÛŒØ¯Ø§Ø¯ Ù‡Ø¯Ù: {dataset[0]['target_event']}\")\n",
    "    print(f\"ğŸ”¹ Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§:\")\n",
    "    for key, val in dataset[0]['options'].items():\n",
    "        print(f\"   {key}: {val}\")\n",
    "    print(f\"ğŸ”¹ Ù¾Ø§Ø³Ø® Ø¯Ø±Ø³Øª: {dataset[0]['golden_answer']}\")\n",
    "    print(f\"ğŸ”¹ Ø·ÙˆÙ„ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ (Context): {len(dataset[0]['context'])} Ú©Ø§Ø±Ø§Ú©ØªØ±\")\n",
    "else:\n",
    "    print(\"Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T18:36:05.592701Z",
     "iopub.status.busy": "2026-02-22T18:36:05.592337Z",
     "iopub.status.idle": "2026-02-22T18:38:08.662089Z",
     "shell.execute_reply": "2026-02-22T18:38:08.661441Z",
     "shell.execute_reply.started": "2026-02-22T18:36:05.592679Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Embedding (Ø¨Ø±Ø§ÛŒ RAG Ù…Ø¹Ù†Ø§ÛŒÛŒ)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90b58c33fb84cfaa943b9ae18ee8c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5779b2f461c4bcb9e4a1ec075ec1461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4476441b786c48b4bdc1439d1bd69c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b56fee46b84d45ad93b05255f6ab14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944f5b7cc4f94b6c8c94bcc2b57dcdc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b30eef633e486cad7bb4e9aa040844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2d8def06b04c46bf63686aa8cca877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b793265c58f24f68a2bfb694eac7f71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e6ea703c75479596b350c71b7c540d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567c2e3edf55443b94ff915245823e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1178b839ac924c77a8a344080de8b6b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21581763f35848acafde0a3e4fed05fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ Ø§ØµÙ„ÛŒ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcf4ab242ad4952a990bd2891287784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2202aa11541f4d25b9eb789340195b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be4a9d632194323b9c1205d911a3b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ae8a2e49f149f89f7e14fb99f16d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf39b1dbfdb446fb0b6ce63be0848a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f34ba7ef0645a7b3715132db3e2bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fbab385b60413e89f1bf7ff1530e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fff51833c7a45289791a33cdd2a4505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be14875292ed438fb37a4a8e51da9093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02957cbace3143bcabae99c6381a584e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'temperature', 'max_new_tokens', 'do_sample', 'pad_token_id'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® (Ø¨Ø§ Ø§Ø³ØªØ¯Ù„Ø§Ù„)...\n",
      "--------------------------------------------------\n",
      "ğŸ§  Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„:\n",
      "The most likely direct cause of the videos of the assassination circulating on social media is that someone recorded the event and shared it online. This action is a result of witnessing the event or obtaining footage from another source.\n",
      "\n",
      "<ANSWER>B</ANSWER>\n",
      "--------------------------------------------------\n",
      "âœ¨ Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: B\n",
      "ğŸ¯ Ù¾Ø§Ø³Ø® Ø¯Ø±Ø³Øª (Golden Answer): D\n"
     ]
    }
   ],
   "source": [
    "# Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯:\n",
    "# !pip install transformers torch scikit-learn sentence-transformers\n",
    "\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ==========================================\n",
    "# Û°. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ ØªØ¹Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ (Embedding) Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ\n",
    "# ==========================================\n",
    "print(\"Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Embedding (Ø¨Ø±Ø§ÛŒ RAG Ù…Ø¹Ù†Ø§ÛŒÛŒ)...\")\n",
    "# ÛŒÚ© Ù…Ø¯Ù„ Ø³Ø¨Ú©ØŒ Ø³Ø±ÛŒØ¹ Ùˆ Ø¨Ø³ÛŒØ§Ø± Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± Ù…ÙÙ‡ÙˆÙ…ÛŒ\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# ==========================================\n",
    "# Û±. ØªØ§Ø¨Ø¹ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª (Semantic RAG) Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø§Ù†ØªÚ©Ø³Øª\n",
    "# ==========================================\n",
    "def retrieve_relevant_context(sample, top_k=5):\n",
    "    \"\"\"\n",
    "    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¬Ù…Ù„Ø§Øª Ú©Ø§Ù†ØªÚ©Ø³Øª Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø¨Ø§Ù‡Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ (Semantic Similarity) Ø¨Ø§ Ú©ÙˆØ¦Ø±ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
    "    \"\"\"\n",
    "    context = sample['context']\n",
    "    \n",
    "    # Ø´Ú©Ø³ØªÙ† Ù…ØªÙ† Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª Ù…Ø¬Ø²Ø§\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', context)\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 20]\n",
    "    \n",
    "    if not sentences:\n",
    "        return \"\"\n",
    "\n",
    "    # Ø³Ø§Ø®Øª Ú©ÙˆØ¦Ø±ÛŒ: Ø±ÙˆÛŒØ¯Ø§Ø¯ Ù‡Ø¯Ù + ØªÙ…Ø§Ù… Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§\n",
    "    query = sample['target_event'] + \" \" + \" \".join(sample['options'].values())\n",
    "\n",
    "    # ØªØ¨Ø¯ÛŒÙ„ Ø¬Ù…Ù„Ø§Øª Ùˆ Ú©ÙˆØ¦Ø±ÛŒ Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ\n",
    "    sentence_embeddings = embedder.encode(sentences)\n",
    "    query_embedding = embedder.encode([query])\n",
    "\n",
    "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ Ø¨ÛŒÙ† Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§\n",
    "    similarities = cosine_similarity(query_embedding, sentence_embeddings).flatten()\n",
    "    \n",
    "    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…Ù„Ø§Øª Ø¨Ø±ØªØ± Ùˆ Ø­ÙØ¸ ØªØ±ØªÛŒØ¨ Ø²Ù…Ø§Ù†ÛŒ Ø¢Ù†â€ŒÙ‡Ø§\n",
    "    top_indices = similarities.argsort()[-top_k:]\n",
    "    top_indices_sorted = sorted(top_indices) \n",
    "    \n",
    "    best_sentences = [sentences[i] for i in top_indices_sorted]\n",
    "    return \" \".join(best_sentences)\n",
    "\n",
    "# ==========================================\n",
    "# Û². ØªØ§Ø¨Ø¹ Ø³Ø§Ø®Øª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Chain of Thought (CoT)\n",
    "# ==========================================\n",
    "def generate_messages(sample, filtered_context):\n",
    "    \"\"\"\n",
    "    Ø¯Ø± Ø§ÛŒÙ† Ù¾Ø±Ø§Ù…Ù¾ØªØŒ Ù…Ø¯Ù„ ØªØ´ÙˆÛŒÙ‚ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø§Ø¨ØªØ¯Ø§ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©Ù†Ø¯ Ùˆ Ø³Ù¾Ø³ Ø¬ÙˆØ§Ø¨ Ø±Ø§ Ø¯Ø± ØªÚ¯ <ANSWER> Ù‚Ø±Ø§Ø± Ø¯Ù‡Ø¯.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are an expert AI specialized in Abductive Event Reasoning. First, briefly reason about the direct cause. Then, you MUST output the uppercase letter(s) of the correct option(s) inside XML tags like this: <ANSWER>A</ANSWER> or <ANSWER>A,C</ANSWER>. Do not output anything else after the tags.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"\"\"Context:\n",
    "{filtered_context}\n",
    "\n",
    "Target Event: {sample['target_event']}\n",
    "\n",
    "Options:\n",
    "A) {sample['options']['A']}\n",
    "B) {sample['options']['B']}\n",
    "C) {sample['options']['C']}\n",
    "D) {sample['options']['D']}\n",
    "\n",
    "Which option(s) is/are the most likely direct cause of the Target Event? Provide a brief reasoning, then wrap your final answer in <ANSWER> tags.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "# ==========================================\n",
    "# Û³. Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…Ø¯Ù„ (LLM Inference)\n",
    "# ==========================================\n",
    "print(\"\\nØ¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ Ø§ØµÙ„ÛŒ...\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"Qwen/Qwen2.5-7B-Instruct\", \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 \n",
    ")\n",
    "print(\"âœ… Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯.\\n\")\n",
    "\n",
    "# ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ù…ØªØºÛŒØ± dataset Ø§Ø² Ø§Ø¬Ø±Ø§ÛŒ Ú¯Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª\n",
    "sample_data = dataset[0]\n",
    "\n",
    "smart_context = retrieve_relevant_context(sample_data, top_k=5)\n",
    "messages = generate_messages(sample_data, smart_context)\n",
    "formatted_prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"\\nØ¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® (Ø¨Ø§ Ø§Ø³ØªØ¯Ù„Ø§Ù„)...\")\n",
    "response = pipe(\n",
    "    formatted_prompt, \n",
    "    max_new_tokens=200, # Ø§ÙØ²Ø§ÛŒØ´ ÙØ¶Ø§ Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ´ØªÙ† Ø§Ø³ØªØ¯Ù„Ø§Ù„\n",
    "    temperature=0.1,    # Ø¯Ù…Ø§ÛŒ Ù¾Ø§ÛŒÛŒÙ† Ø¨Ø±Ø§ÛŒ Ù…Ù†Ø·Ù‚ Ø§Ø³ØªÙˆØ§Ø±\n",
    "    do_sample=True,\n",
    "    pad_token_id=pipe.tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_text = response[0]['generated_text']\n",
    "raw_answer = generated_text.replace(formatted_prompt, \"\").strip()\n",
    "\n",
    "# Øª) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡ Ø§Ø² Ø¯Ø§Ø®Ù„ ØªÚ¯ <ANSWER>\n",
    "match = re.search(r'<ANSWER>\\s*([A-D](?:\\s*,\\s*[A-D])*)\\s*</ANSWER>', raw_answer)\n",
    "if match:\n",
    "    clean_answer = match.group(1).replace(' ', '')\n",
    "else:\n",
    "    # ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ (Fallback) Ø§Ú¯Ø± Ù…Ø¯Ù„ ØªÚ¯â€ŒÙ‡Ø§ Ø±Ø§ Ø±Ø¹Ø§ÛŒØª Ù†Ú©Ø±Ø¯\n",
    "    fallback_match = re.search(r'[A-D](?:\\s*,\\s*[A-D])*', raw_answer)\n",
    "    clean_answer = fallback_match.group(0).replace(' ', '') if fallback_match else \"Ù†Ø§Ù…Ø´Ø®Øµ\"\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"ğŸ§  Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„:\\n{raw_answer}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"âœ¨ Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {clean_answer}\")\n",
    "print(f\"ğŸ¯ Ù¾Ø§Ø³Ø® Ø¯Ø±Ø³Øª (Golden Answer): {sample_data['golden_answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T18:38:08.663390Z",
     "iopub.status.busy": "2026-02-22T18:38:08.662982Z",
     "iopub.status.idle": "2026-02-22T19:00:24.511867Z",
     "shell.execute_reply": "2026-02-22T19:00:24.511287Z",
     "shell.execute_reply.started": "2026-02-22T18:38:08.663365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ø´Ø±ÙˆØ¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ 200 Ù†Ù…ÙˆÙ†Ù‡...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Samples:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   0%|          | 1/200 [00:03<12:32,  3.78s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   1%|          | 2/200 [00:10<17:50,  5.41s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   2%|â–         | 3/200 [00:17<20:27,  6.23s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   2%|â–         | 4/200 [00:21<16:55,  5.18s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   2%|â–         | 5/200 [00:26<17:11,  5.29s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   3%|â–         | 6/200 [00:36<22:40,  7.01s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   4%|â–         | 7/200 [00:41<20:07,  6.26s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   4%|â–         | 8/200 [00:48<20:43,  6.48s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   4%|â–         | 9/200 [00:55<21:01,  6.60s/it]\u001b[AYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   5%|â–Œ         | 10/200 [01:02<21:23,  6.76s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   6%|â–Œ         | 11/200 [01:08<20:01,  6.36s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   6%|â–Œ         | 12/200 [01:14<20:13,  6.45s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   6%|â–‹         | 13/200 [01:19<19:00,  6.10s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   7%|â–‹         | 14/200 [01:29<21:41,  7.00s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   8%|â–Š         | 15/200 [01:34<20:11,  6.55s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   8%|â–Š         | 16/200 [01:41<20:33,  6.71s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   8%|â–Š         | 17/200 [01:48<20:14,  6.64s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:   9%|â–‰         | 18/200 [01:54<20:19,  6.70s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  10%|â–‰         | 19/200 [02:00<19:31,  6.47s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  10%|â–ˆ         | 20/200 [02:05<18:02,  6.01s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  10%|â–ˆ         | 21/200 [02:10<16:25,  5.50s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  11%|â–ˆ         | 22/200 [02:19<20:06,  6.78s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  12%|â–ˆâ–        | 23/200 [02:27<21:04,  7.14s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  12%|â–ˆâ–        | 24/200 [02:34<20:04,  6.84s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  12%|â–ˆâ–        | 25/200 [02:42<21:03,  7.22s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  13%|â–ˆâ–        | 26/200 [02:47<19:38,  6.77s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  14%|â–ˆâ–        | 27/200 [02:54<19:11,  6.66s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  14%|â–ˆâ–        | 28/200 [02:59<17:36,  6.15s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  14%|â–ˆâ–        | 29/200 [03:05<17:35,  6.17s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  15%|â–ˆâ–Œ        | 30/200 [03:16<21:32,  7.60s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  16%|â–ˆâ–Œ        | 31/200 [03:21<19:36,  6.96s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  16%|â–ˆâ–Œ        | 32/200 [03:30<21:12,  7.58s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  16%|â–ˆâ–‹        | 33/200 [03:37<20:30,  7.37s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  17%|â–ˆâ–‹        | 34/200 [03:44<19:46,  7.15s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  18%|â–ˆâ–Š        | 35/200 [03:49<17:51,  6.50s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  18%|â–ˆâ–Š        | 36/200 [03:54<17:01,  6.23s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  18%|â–ˆâ–Š        | 37/200 [04:03<18:59,  6.99s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  19%|â–ˆâ–‰        | 38/200 [04:10<18:45,  6.95s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  20%|â–ˆâ–‰        | 39/200 [04:15<16:47,  6.26s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  20%|â–ˆâ–ˆ        | 40/200 [04:21<16:58,  6.37s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  20%|â–ˆâ–ˆ        | 41/200 [04:28<17:01,  6.43s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  21%|â–ˆâ–ˆ        | 42/200 [04:39<20:54,  7.94s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  22%|â–ˆâ–ˆâ–       | 43/200 [04:45<18:41,  7.14s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  22%|â–ˆâ–ˆâ–       | 44/200 [04:52<18:58,  7.30s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  22%|â–ˆâ–ˆâ–       | 45/200 [04:59<18:40,  7.23s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  23%|â–ˆâ–ˆâ–       | 46/200 [05:05<17:01,  6.63s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  24%|â–ˆâ–ˆâ–       | 47/200 [05:09<15:19,  6.01s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  24%|â–ˆâ–ˆâ–       | 48/200 [05:14<14:18,  5.65s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  24%|â–ˆâ–ˆâ–       | 49/200 [05:21<14:53,  5.92s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [05:29<16:58,  6.79s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [05:34<15:02,  6.06s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [05:39<14:23,  5.83s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  26%|â–ˆâ–ˆâ–‹       | 53/200 [05:52<19:25,  7.93s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  27%|â–ˆâ–ˆâ–‹       | 54/200 [05:57<17:36,  7.23s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  28%|â–ˆâ–ˆâ–Š       | 55/200 [06:03<16:27,  6.81s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  28%|â–ˆâ–ˆâ–Š       | 56/200 [06:09<15:38,  6.52s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  28%|â–ˆâ–ˆâ–Š       | 57/200 [06:18<16:56,  7.11s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  29%|â–ˆâ–ˆâ–‰       | 58/200 [06:23<15:25,  6.52s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  30%|â–ˆâ–ˆâ–‰       | 59/200 [06:27<13:31,  5.75s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [06:31<12:43,  5.45s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [06:38<13:32,  5.84s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [06:46<14:39,  6.37s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [06:54<15:40,  6.87s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [07:00<14:58,  6.61s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  32%|â–ˆâ–ˆâ–ˆâ–      | 65/200 [07:09<16:46,  7.46s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  33%|â–ˆâ–ˆâ–ˆâ–      | 66/200 [07:16<15:56,  7.14s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  34%|â–ˆâ–ˆâ–ˆâ–      | 67/200 [07:28<19:04,  8.60s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [07:34<17:42,  8.05s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [07:41<16:16,  7.46s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [07:51<18:00,  8.31s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [08:01<18:49,  8.75s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [08:07<17:00,  7.97s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [08:13<15:33,  7.35s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [08:18<14:22,  6.84s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [08:23<12:49,  6.16s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [08:28<12:00,  5.81s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [08:35<12:36,  6.15s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [08:39<11:17,  5.55s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [08:47<12:33,  6.23s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [08:52<11:42,  5.85s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [08:59<12:09,  6.13s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [09:04<11:35,  5.90s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [09:08<10:29,  5.38s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [09:16<11:59,  6.20s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/200 [09:22<11:42,  6.10s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 86/200 [09:27<11:09,  5.87s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 87/200 [09:32<10:05,  5.36s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [09:36<09:31,  5.10s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [09:46<11:54,  6.44s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [09:52<11:35,  6.32s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [09:58<11:33,  6.36s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [10:03<10:43,  5.96s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [10:10<10:51,  6.09s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [10:14<09:57,  5.64s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [10:20<09:55,  5.67s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [10:25<09:24,  5.43s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [10:30<09:19,  5.43s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [10:37<10:07,  5.96s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [10:45<10:54,  6.48s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [10:50<09:48,  5.89s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [10:57<10:39,  6.46s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [11:02<09:31,  5.83s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [11:09<10:21,  6.41s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [11:17<10:34,  6.61s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 105/200 [11:22<09:43,  6.14s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 106/200 [11:28<09:38,  6.15s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 107/200 [11:35<10:07,  6.54s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [11:41<09:38,  6.29s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [11:47<09:31,  6.28s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [11:51<08:09,  5.44s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [11:58<08:46,  5.91s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [12:03<08:32,  5.83s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [12:10<08:52,  6.12s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [12:15<08:12,  5.72s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [12:30<11:56,  8.43s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [12:36<10:45,  7.68s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [12:41<09:34,  6.92s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [12:49<10:04,  7.37s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [12:55<09:27,  7.00s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [13:03<09:36,  7.20s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [13:12<10:23,  7.89s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [13:18<09:13,  7.09s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [13:24<08:51,  6.90s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [13:29<08:03,  6.37s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 125/200 [13:45<11:17,  9.04s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 126/200 [13:50<09:41,  7.86s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 127/200 [13:59<10:09,  8.35s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [14:07<09:59,  8.33s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [14:15<09:25,  7.96s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [14:19<08:01,  6.88s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [14:28<08:40,  7.55s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [14:33<07:33,  6.66s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [14:39<07:25,  6.65s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [14:47<07:43,  7.02s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [14:52<06:57,  6.42s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [14:59<07:03,  6.61s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [15:06<07:07,  6.78s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [15:11<06:26,  6.23s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [15:18<06:20,  6.24s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [15:24<06:09,  6.16s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [15:30<06:15,  6.36s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [15:39<06:44,  6.98s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [15:44<06:09,  6.48s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [15:54<07:04,  7.58s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 145/200 [16:00<06:20,  6.93s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 146/200 [16:04<05:36,  6.22s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/200 [16:11<05:45,  6.52s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [16:18<05:35,  6.45s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [16:29<06:39,  7.83s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [16:34<05:55,  7.11s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [16:40<05:32,  6.79s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [16:46<05:09,  6.44s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [16:52<05:02,  6.43s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [16:59<05:05,  6.64s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [17:06<05:02,  6.73s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [17:18<06:03,  8.27s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [17:24<05:22,  7.50s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [17:29<04:43,  6.74s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [17:34<04:14,  6.20s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [17:39<03:59,  5.98s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [17:47<04:14,  6.52s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [17:54<04:17,  6.77s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [18:04<04:36,  7.47s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [18:08<03:56,  6.56s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 165/200 [18:13<03:29,  5.99s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 166/200 [18:20<03:35,  6.33s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 167/200 [18:25<03:16,  5.95s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [18:30<03:01,  5.68s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [18:35<02:49,  5.48s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [18:41<02:50,  5.68s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [18:48<02:52,  5.94s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [18:53<02:38,  5.65s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [18:58<02:34,  5.73s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [19:04<02:30,  5.79s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [19:14<02:49,  6.79s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [19:19<02:32,  6.36s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [19:29<02:53,  7.54s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [19:37<02:47,  7.59s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [19:43<02:32,  7.28s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [19:48<02:09,  6.49s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [19:54<02:02,  6.42s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [20:01<01:57,  6.54s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [20:10<02:01,  7.14s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [20:15<01:46,  6.64s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 185/200 [20:24<01:48,  7.23s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 186/200 [20:28<01:29,  6.36s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 187/200 [20:33<01:16,  5.89s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [20:42<01:22,  6.84s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [20:51<01:21,  7.45s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [20:57<01:10,  7.07s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [21:05<01:05,  7.23s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [21:10<00:53,  6.67s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [21:15<00:44,  6.31s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [21:23<00:39,  6.63s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [21:30<00:34,  6.82s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [21:44<00:36,  9.03s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [21:55<00:28,  9.65s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [22:02<00:17,  8.82s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [22:09<00:08,  8.11s/it]\u001b[ABoth `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\n",
      "Processing Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [22:15<00:00,  6.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ† Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ (Semantic RAG + CoT) ğŸ†\n",
      "==================================================\n",
      "ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: 200\n",
      "Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ú©Ø³Ø¨ Ø´Ø¯Ù‡: 122.5\n",
      "Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² (Accuracy): 0.6125 ÛŒØ§ 61.25%\n",
      "==================================================\n",
      "\n",
      "ğŸ” ØªØ­Ù„ÛŒÙ„ Ø®Ø·Ø§Ù‡Ø§ (Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² Û°):\n",
      "ID: q-6 | Predicted: D | Golden: C\n",
      "ID: q-9 | Predicted: B | Golden: D\n",
      "ID: q-10 | Predicted: B | Golden: A,C\n",
      "ID: q-12 | Predicted: C | Golden: A\n",
      "ID: q-16 | Predicted: C | Golden: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# ØªØ§Ø¨Ø¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
    "# ==========================================\n",
    "def calculate_score(predicted_answers_str, golden_answers_str):\n",
    "    if predicted_answers_str == \"Ù†Ø§Ù…Ø´Ø®Øµ\":\n",
    "        return 0.0\n",
    "        \n",
    "    P = set([ans.strip() for ans in predicted_answers_str.split(',') if ans.strip()])\n",
    "    G = set([ans.strip() for ans in golden_answers_str.split(',') if ans.strip()])\n",
    "    \n",
    "    if P == G and len(P) > 0:\n",
    "        return 1.0  \n",
    "    elif P.issubset(G) and len(P) > 0:\n",
    "        return 0.5  \n",
    "    else:\n",
    "        return 0.0  \n",
    "\n",
    "# ==========================================\n",
    "# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù„ÛŒ Ø±ÙˆÛŒ Ú©Ù„ Ø¯ÛŒØªØ§Ø³Øª (Ø¨Ø§ Semantic RAG Ùˆ CoT)\n",
    "# ==========================================\n",
    "print(f\"\\nØ´Ø±ÙˆØ¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ {len(dataset)} Ù†Ù…ÙˆÙ†Ù‡...\")\n",
    "total_score = 0\n",
    "results_log = []\n",
    "\n",
    "for index, sample in enumerate(tqdm(dataset, desc=\"Processing Samples\")):\n",
    "    \n",
    "    # Û±. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù†ØªÚ©Ø³Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ\n",
    "    smart_context = retrieve_relevant_context(sample, top_k=5)\n",
    "    \n",
    "    # Û². Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª\n",
    "    messages = generate_messages(sample, smart_context)\n",
    "    formatted_prompt = pipe.tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Û³. ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒ (Ø¨Ø§ ÙØ¶Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ¯Ù„Ø§Ù„)\n",
    "    response = pipe(\n",
    "        formatted_prompt, \n",
    "        max_new_tokens=200, \n",
    "        temperature=0.1, \n",
    "        do_sample=True,\n",
    "        pad_token_id=pipe.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = response[0]['generated_text']\n",
    "    raw_answer = generated_text.replace(formatted_prompt, \"\").strip()\n",
    "    \n",
    "    # Û´. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² ØªÚ¯ ANSWER\n",
    "    match = re.search(r'<ANSWER>\\s*([A-D](?:\\s*,\\s*[A-D])*)\\s*</ANSWER>', raw_answer)\n",
    "    if match:\n",
    "        clean_answer = match.group(1).replace(' ', '')\n",
    "    else:\n",
    "        fallback_match = re.search(r'[A-D](?:\\s*,\\s*[A-D])*', raw_answer)\n",
    "        clean_answer = fallback_match.group(0).replace(' ', '') if fallback_match else \"Ù†Ø§Ù…Ø´Ø®Øµ\"\n",
    "    \n",
    "    # Ûµ. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²\n",
    "    golden = sample['golden_answer']\n",
    "    score = calculate_score(clean_answer, golden)\n",
    "    total_score += score\n",
    "    \n",
    "    results_log.append({\n",
    "        \"id\": sample['id'],\n",
    "        \"target_event\": sample['target_event'],\n",
    "        \"predicted\": clean_answer,\n",
    "        \"golden\": golden,\n",
    "        \"score\": score\n",
    "    })\n",
    "\n",
    "# ==========================================\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "# ==========================================\n",
    "average_score = total_score / len(dataset)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ† Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ (Semantic RAG + CoT) ğŸ†\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {len(dataset)}\")\n",
    "print(f\"Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ú©Ø³Ø¨ Ø´Ø¯Ù‡: {total_score}\")\n",
    "print(f\"Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² (Accuracy): {average_score:.4f} ÛŒØ§ {average_score * 100:.2f}%\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nğŸ” ØªØ­Ù„ÛŒÙ„ Ø®Ø·Ø§Ù‡Ø§ (Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² Û°):\")\n",
    "error_count = 0\n",
    "for res in results_log:\n",
    "    if res['score'] == 0.0:\n",
    "        print(f\"ID: {res['id']} | Predicted: {res['predicted']} | Golden: {res['golden']}\")\n",
    "        error_count += 1\n",
    "        if error_count >= 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
